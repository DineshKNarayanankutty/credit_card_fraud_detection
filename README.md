# Credit Card Fraud Detection â€” Production-Grade MLOps System

This repository demonstrates how a **machine learning model is built, tracked, registered, and served as a production system**, not just a notebook experiment.

The project focuses on **end-to-end MLOps practices** using **DVC, MLflow, Azure ML, and FastAPI**, with a strong emphasis on **reproducibility, model governance, and cloud deployment**.
The ML model itself is intentionally simple so the focus remains on **system design, reliability, and production readiness**.

---

## ğŸš€ Project Objectives

* Build a **fully reproducible ML pipeline** (data â†’ model â†’ evaluation)
* Track **data, parameters, and artifacts** using DVC
* Log experiments, metrics, and artifacts with **MLflow**
* Train and **register models in Azure ML**
* Serve predictions through a **containerized FastAPI service**
* Deploy inference to **Azure Web App for Containers**
* Follow **production MLOps design principles**

---

## ğŸ§  Problem Statement

Credit card fraud detection is a **highly imbalanced classification problem** (~0.2% fraud), where:

* Accuracy alone is misleading
* Recallâ€“precision trade-offs are critical
* Threshold tuning directly impacts business outcomes
* Data behavior can change over time in production

This project treats fraud detection as a **system engineering problem**, not just a modeling exercise.

---

## ğŸ—ï¸ High-Level Architecture

```
Raw Data (DVC)
   â†“
Preprocessing Pipeline
   â†“
Train / Validation / Test Split
   â†“
Model Training + Cross-Validation
   â†“
Threshold Optimization
   â†“
Evaluation & Reporting
   â†“
Model + Artifacts (DVC + MLflow)
   â†“
Azure ML Model Registry
   â†“
FastAPI Inference Service
   â†“
Azure Web App (Containerized Deployment)
```

---

## ğŸ“‚ Repository Structure

```
credit_card_fraud_detection/
â”‚
â”œâ”€â”€ api/                     # FastAPI inference service
â”‚
â”œâ”€â”€ pipelines/               # Pipeline orchestration (DVC / Azure ML ready)
â”‚   â”œâ”€â”€ train_pipeline.py
â”‚   â””â”€â”€ evaluate_pipeline.py
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/                # Data loading, cleaning, splitting
â”‚   â”œâ”€â”€ features/            # Imbalance handling (SMOTE)
â”‚   â”œâ”€â”€ models/              # Model factory, training, registry
â”‚   â”œâ”€â”€ evaluation/          # Metrics, thresholding, reports
â”‚   â”œâ”€â”€ inference/           # Prediction abstractions
â”‚   â””â”€â”€ utils/               # Config, logging, IO
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # Raw dataset (DVC tracked)
â”‚   â””â”€â”€ processed/           # Train/val/test splits (DVC outputs)
â”‚
â”œâ”€â”€ artifacts/               # Model, scaler, metrics (DVC outputs)
â”œâ”€â”€ reports/                 # Evaluation reports
â”‚
â”œâ”€â”€ dvc.yaml                 # DVC pipeline definition
â”œâ”€â”€ dvc.lock                 # Reproducibility lockfile
â”œâ”€â”€ params.yaml              # Tunable ML parameters
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ” Pipeline Stages (DVC)

### 1ï¸âƒ£ Preprocessing

* Load raw dataset
* Clean data (missing values, duplicates)
* Perform **leakage-safe train/validation/test split**
* Fit scaler **only on training data**
* Persist processed data and scaler

```bash
dvc repro preprocess
```

---

### 2ï¸âƒ£ Training

* Load processed datasets
* Handle class imbalance using **SMOTE (train only)**
* Train model and perform cross-validation
* Optimize decision threshold using validation data
* Evaluate on test set
* Log metrics and artifacts to **MLflow**
* Save artifacts via **DVC**
* Register model in **Azure ML Model Registry**

```bash
dvc repro train
```

---

### 3ï¸âƒ£ Evaluation

* Load trained model and scaler
* Run threshold-based evaluation
* Generate human-readable evaluation reports
* Persist evaluation metrics

```bash
dvc repro evaluate
```

---

## ğŸ“Š Experiment Tracking (MLflow)

MLflow is used to:

* Log model parameters and hyperparameters
* Track metrics (precision, recall, F1, ROC-AUC, PR-AUC)
* Store artifacts (model, scaler)
* Enable comparison across runs
* Support both **local and Azure ML-backed tracking**

Launch MLflow UI locally:

```bash
mlflow ui
```

---

## â˜ï¸ Azure ML Integration

* Training pipeline is **Azure ML compatible**
* Models are **registered in Azure ML Model Registry**
* Artifacts follow Azure ML output conventions
* Enables versioned, auditable model promotion

---

## ğŸš€ Inference & Deployment

* Built a **FastAPI-based inference service**
* Supports single and batch predictions
* Loads model and scaler dynamically
* Containerized using Docker
* Deployed on **Azure Web App for Containers**
* Ready for horizontal scaling and CI/CD integration

---

## ğŸ” Design Principles

* Strict **separation of concerns**
* Pipelines orchestrate, modules implement logic
* No data leakage
* Deterministic, reproducible runs
* Cloud-first but cloud-agnostic structure
* Production-readiness over experimentation

---

## ğŸ§© Why This Project Matters

This repository demonstrates:

* Real-world MLOps engineering practices
* Proper handling of highly imbalanced data
* Reproducible ML pipelines with DVC
* Experiment tracking and model governance with MLflow & Azure ML
* End-to-end deployment from training to live inference

---

## ğŸ”œ Potential Extensions

* Azure ML Jobs for fully managed training
* CI/CD pipeline for model promotion
* Centralized MLflow tracking backend
* Live monitoring and drift dashboards
* AKS-based inference deployment
