Credit Card Fraud Detection â€” Production-Grade MLOps Pipeline

This repository demonstrates how a machine learning model is built, versioned, evaluated, and monitored as a production system, not just a notebook experiment.

The project focuses on end-to-end MLOps practices using DVC, MLflow, modular pipelines, and drift-ready architecture, while keeping the ML model intentionally simple so the emphasis remains on system design and reliability.

ğŸš€ Project Goals

Build a reproducible ML training pipeline

Track data, parameters, and artifacts using DVC

Log experiments and metrics with MLflow

Serve models via clean inference abstractions

Prepare the system for production monitoring and drift detection

Keep the architecture cloud-ready (Azure ML compatible)

ğŸ§  Problem Statement

Credit card fraud detection is a highly imbalanced classification problem where:

Accuracy alone is misleading

Recall and precision trade-offs matter

Threshold tuning is critical

Data drift is common in production

This project treats fraud detection as a system problem, not just a modeling task.

ğŸ—ï¸ High-Level Architecture
Raw Data (DVC)
   â†“
Preprocessing Pipeline
   â†“
Train / Validate / Test Split
   â†“
Model Training + Cross-Validation
   â†“
Threshold Optimization
   â†“
Evaluation & Reporting
   â†“
Model + Artifacts (DVC + MLflow)
   â†“
Inference / Monitoring (Drift-Ready)

ğŸ“‚ Repository Structure
credit_card_fraud_detection/
â”‚
â”œâ”€â”€ api/                     # FastAPI inference service
â”‚
â”œâ”€â”€ pipelines/               # Orchestration only (DVC / Azure ML ready)
â”‚   â”œâ”€â”€ train_pipeline.py
â”‚   â”œâ”€â”€ evaluate_pipeline.py
â”‚   â””â”€â”€ drift_pipeline.py
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/                # Data loading, cleaning, splitting
â”‚   â”œâ”€â”€ features/            # Feature engineering (imbalance handling)
â”‚   â”œâ”€â”€ models/              # Model factory, training, registry
â”‚   â”œâ”€â”€ evaluation/          # Metrics, thresholding, reports
â”‚   â”œâ”€â”€ inference/           # Prediction abstractions
â”‚   â”œâ”€â”€ monitoring/          # Drift detection contract
â”‚   â””â”€â”€ utils/               # Config, logging, IO
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # Raw datasets (DVC tracked)
â”‚   â”œâ”€â”€ processed/           # Train/val/test splits (DVC outputs)
â”‚   â”œâ”€â”€ reference/           # Baseline data for drift checks
â”‚   â””â”€â”€ incoming/            # New production data
â”‚
â”œâ”€â”€ artifacts/               # Models, scalers, metrics (DVC outputs)
â”œâ”€â”€ reports/                 # Human-readable evaluation & drift reports
â”‚
â”œâ”€â”€ dvc.yaml                 # Pipeline definition
â”œâ”€â”€ dvc.lock                 # Reproducibility lockfile
â”œâ”€â”€ params.yaml              # Tunable ML parameters
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸ” Pipeline Stages (DVC)
1ï¸âƒ£ Preprocessing

Load raw CSV

Clean data (missing values, duplicates, outliers)

Train / validation / test split

Fit scaler on train only

Persist splits and scaler

dvc repro preprocess

2ï¸âƒ£ Training

Load processed data

Handle class imbalance (SMOTE on train only)

Train model

Cross-validate

Optimize decision threshold

Evaluate on test set

Save model and metrics

dvc repro train

3ï¸âƒ£ Evaluation

Load trained model and scaler

Run threshold analysis

Generate evaluation reports

Persist metrics for tracking

dvc repro evaluate

4ï¸âƒ£ Drift Detection (Contract-Based)

Checks for presence of reference & incoming data

Emits drift signals without breaking pipelines

Designed for post-deployment monitoring tools

Compatible with Evidently / Azure ML Monitoring

dvc repro drift_check


Drift detection is intentionally decoupled from training to keep pipelines deterministic and production-safe.

ğŸ“Š Experiment Tracking (MLflow)

MLflow is used to:

Track metrics and parameters

Compare experiment runs

Prepare for remote tracking backends (Azure ML)

Start UI locally:

mlflow ui

ğŸ§ª Model Performance (Example)

Imbalanced dataset (~0.2% fraud)

Accuracy alone is misleading

Threshold tuning improves recall

Evaluation focuses on:

Precision

Recall

F1-score

ROC-AUC

PR-AUC

ğŸ” Design Principles

Separation of concerns

Pipelines orchestrate

Modules implement logic

No data leakage

Reproducibility first

Monitoring â‰  Training

Cloud-agnostic by default

â˜ï¸ Cloud & Deployment Readiness

This project is intentionally structured to support:

Azure ML Jobs

Azure Blob Storage (DVC remote)

AKS / Container deployment

Production monitoring tools

Next phase: Azure ML integration for training orchestration and registry.

ğŸ§© Why This Project Matters

This repository demonstrates:

Real MLOps engineering (not tutorials)

Correct handling of imbalanced data

Clean pipeline orchestration

Drift-ready system design

Interview-grade architecture decisions

ğŸ”œ Next Steps

Integrate Azure ML training jobs

Configure MLflow remote backend

Containerize inference service

Deploy to AKS

Add live monitoring dashboards
